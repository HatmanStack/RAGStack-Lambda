# RAGStack-Lambda GraphQL Schema

type Query {
  # Get document by ID
  getDocument(documentId: ID!): Document

  # List all documents (paginated)
  listDocuments(limit: Int, nextToken: String): DocumentConnection

  # Query Knowledge Base with optional session for conversation continuity
  queryKnowledgeBase(query: String!, sessionId: String): ChatResponse

  # Search Knowledge Base and return raw vector search results
  searchKnowledgeBase(query: String!, maxResults: Int): KBQueryResult

  # Get system configuration (Schema, Default, and Custom)
  getConfiguration: ConfigurationResponse

  # Get theme configuration (public, no auth required)
  # Returns only theme-related settings for the chat web component
  getThemeConfig: ThemeConfigResponse @aws_api_key @aws_iam

  # Get scrape job by ID with pages
  getScrapeJob(jobId: ID!): ScrapeJobDetail

  # List all scrape jobs (paginated)
  listScrapeJobs(limit: Int, nextToken: String): ScrapeJobConnection

  # Check if a URL has been scraped before
  checkScrapeUrl(url: String!): ScrapeUrlCheck
}

type Mutation {
  # Create presigned URL for upload
  createUploadUrl(filename: String!): UploadUrl

  # Manually trigger processing (if needed)
  processDocument(documentId: ID!): Document

  # Update custom configuration
  updateConfiguration(customConfig: AWSJSON!): Boolean

  # Start a new web scraping job
  startScrape(input: StartScrapeInput!): ScrapeJob

  # Cancel an in-progress scrape job
  cancelScrape(jobId: ID!): ScrapeJob
}

# Document type
type Document {
  documentId: ID!
  filename: String!
  inputS3Uri: String!
  outputS3Uri: String
  status: DocumentStatus!
  fileType: String
  isTextNative: Boolean
  totalPages: Int
  errorMessage: String
  createdAt: String
  updatedAt: String
  metadata: AWSJSON
}

# Document status enum
enum DocumentStatus {
  UPLOADED
  PROCESSING
  OCR_COMPLETE
  EMBEDDING_COMPLETE
  INDEXED
  FAILED
}

# Paginated document list
type DocumentConnection {
  items: [Document!]!
  nextToken: String
}

# Upload URL response
type UploadUrl {
  uploadUrl: String!
  documentId: ID!
  fields: AWSJSON
}

# Chat response with session support for conversation continuity
type ChatResponse {
  answer: String!
  sessionId: String
  sources: [Source!]!
  error: String
}

# Source citation from Knowledge Base retrieval
type Source {
  documentId: String!
  pageNumber: Int
  s3Uri: String!
  snippet: String
  documentUrl: String
  documentAccessAllowed: Boolean
}

# Knowledge Base search result (raw vector search)
type KBQueryResult {
  query: String!
  results: [KBResult!]!
  total: Int
  error: String
}

# Individual KB search result
type KBResult {
  content: String!
  source: String
  score: Float
}

# Configuration response containing Schema, Default, and Custom configs
#
# The Schema contains field metadata with these properties:
# - type: Field data type (e.g., "string")
# - enum: Array of allowed values for dropdown fields
# - description: Human-readable field label
# - order: Display order in UI (lower numbers first)
# - dependsOn: Optional conditional rendering {field, value}
#
# Configurable fields:
# 1. ocr_backend: Choose between "textract" or "bedrock" for OCR processing
# 2. bedrock_ocr_model_id: Claude model selection for Bedrock OCR (conditional on ocr_backend="bedrock")
# 3. chat_model_id: Model selection for Knowledge Base chat queries (Nova/Claude options)
#
# Note: Embedding models are hardcoded to Titan defaults and not user-configurable.
type ConfigurationResponse @aws_cognito_user_pools {
  Schema: AWSJSON
  Default: AWSJSON
  Custom: AWSJSON
}

# Theme configuration for chat web component (public access)
type ThemeConfigResponse @aws_api_key @aws_iam {
  themePreset: String!
  primaryColor: String
  fontFamily: String
  spacing: String
}

# =========================================================================
# Scrape Types
# =========================================================================

# Scrape job status enum
enum ScrapeStatus {
  PENDING
  DISCOVERING
  PROCESSING
  COMPLETED
  COMPLETED_WITH_ERRORS
  FAILED
  CANCELLED
}

# URL scope enforcement for crawling
enum ScrapeScope {
  SUBPAGES   # Only URLs under the base path
  HOSTNAME   # Any URL on the same hostname
  DOMAIN     # Any URL on the same domain (includes subdomains)
}

# Scrape mode for content fetching
enum ScrapeMode {
  FAST   # HTTP fetch only
  FULL   # Always use Playwright
  AUTO   # Auto-detect based on content
}

# Scrape job configuration
type ScrapeConfig {
  maxPages: Int!
  maxDepth: Int!
  scope: ScrapeScope!
  includePatterns: [String!]
  excludePatterns: [String!]
  scrapeMode: ScrapeMode
  cookies: String
}

# Input for starting a scrape job
input StartScrapeInput {
  url: String!
  maxPages: Int
  maxDepth: Int
  scope: ScrapeScope
  includePatterns: [String!]
  excludePatterns: [String!]
  scrapeMode: ScrapeMode
  cookies: String
}

# Scrape job type
type ScrapeJob {
  jobId: ID!
  baseUrl: String!
  title: String
  status: ScrapeStatus!
  config: ScrapeConfig!
  totalUrls: Int!
  processedCount: Int!
  failedCount: Int!
  failedUrls: [String!]
  createdAt: AWSDateTime!
  updatedAt: AWSDateTime!
}

# Scrape job with pages
type ScrapeJobDetail {
  job: ScrapeJob!
  pages: [ScrapePage!]!
}

# Individual scraped page
type ScrapePage {
  url: String!
  title: String
  status: String!
  documentId: ID
  error: String
  depth: Int!
}

# Paginated scrape job list
type ScrapeJobConnection {
  items: [ScrapeJob!]!
  nextToken: String
}

# URL check result for duplicate detection
type ScrapeUrlCheck {
  exists: Boolean!
  lastScrapedAt: AWSDateTime
  jobId: ID
  title: String
}

schema {
  query: Query
  mutation: Mutation
}
